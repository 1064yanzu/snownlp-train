# -*- coding: utf-8 -*-
"""
SnowNLPæƒ…æ„Ÿåˆ†æè®­ç»ƒå·¥å…· - å‘½ä»¤è¡Œç‰ˆæœ¬
é€‚ç”¨äºLinuxäº‘ç¯å¢ƒã€æ— å¤´æœåŠ¡å™¨ç­‰æ— å›¾å½¢ç•Œé¢ç¯å¢ƒ
"""

import os
import sys
import time
import shutil
import argparse
import pandas as pd
from glob import glob
from datetime import datetime
import json
import random

def print_banner():
    """æ‰“å°ç¨‹åºæ¨ªå¹…"""
    print("=" * 60)
    print("ğŸš€ SnowNLPæƒ…æ„Ÿåˆ†æè®­ç»ƒå·¥å…· - å‘½ä»¤è¡Œç‰ˆæœ¬")
    print("=" * 60)
    print("ğŸŒŸ ä¸“ä¸ºLinuxäº‘ç¯å¢ƒå’Œæ— å¤´æœåŠ¡å™¨è®¾è®¡")
    print("âš¡ æ”¯æŒå®Œæ•´çš„æ¨¡å‹è®­ç»ƒã€æµ‹è¯•å’Œç®¡ç†åŠŸèƒ½")
    print("=" * 60)

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    print("\nğŸ” æ£€æŸ¥ä¾èµ–åº“...")
    dependencies = {
        'pandas': 'pandas',
        'snownlp': 'snownlp', 
        'tqdm': 'tqdm',
        'numpy': 'numpy',
        'jieba': 'jieba'
    }
    
    missing = []
    for name, module in dependencies.items():
        try:
            __import__(module)
            print(f"âœ… {name} å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {name} æœªå®‰è£…")
            missing.append(module)
    
    if missing:
        print(f"\nğŸ“¦ å®‰è£…ç¼ºå¤±ä¾èµ–...")
        import subprocess
        for module in missing:
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", module])
                print(f"âœ… {module} å®‰è£…æˆåŠŸ")
            except Exception as e:
                print(f"âŒ {module} å®‰è£…å¤±è´¥: {e}")
                return False
    
    print("ğŸ‰ æ‰€æœ‰ä¾èµ–æ£€æŸ¥å®Œæˆ!")
    return True

def find_data_files():
    """æŸ¥æ‰¾æ•°æ®æ–‡ä»¶"""
    print("\nğŸ“ æŸ¥æ‰¾æ•°æ®æ–‡ä»¶...")
    
    # è®­ç»ƒæ–‡ä»¶æ¨¡å¼
    train_patterns = ['train.csv', 'è®­ç»ƒ*.csv', '*train*.csv']
    train_files = []
    for pattern in train_patterns:
        train_files.extend(glob(pattern))
    
    # æµ‹è¯•æ–‡ä»¶æ¨¡å¼
    test_patterns = ['test.csv', 'æµ‹è¯•*.csv', '*test*.csv']
    test_files = []
    for pattern in test_patterns:
        test_files.extend(glob(pattern))
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶")
    for f in train_files:
        size = os.path.getsize(f)
        print(f"  - {f} ({size:,} å­—èŠ‚)")
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
    for f in test_files:
        size = os.path.getsize(f)
        print(f"  - {f} ({size:,} å­—èŠ‚)")
    
    return train_files, test_files

def load_data_with_progress(filepaths, data_type="æ•°æ®", neutral_strategy="balance"):
    """åŠ è½½æ•°æ®å¹¶æ˜¾ç¤ºè¿›åº¦"""
    from tqdm import tqdm
    
    print(f"\nğŸ“‚ åŠ è½½{data_type}æ–‡ä»¶...")
    print(f"ğŸ”§ ä¸­æ€§æ•°æ®å¤„ç†ç­–ç•¥: {neutral_strategy}")
    
    # æ ‡ç­¾æ˜ å°„
    label_mapping = {
        # ä¸­æ–‡æ ‡ç­¾
        'è´Ÿé¢': 0, 'æ¶ˆæ': 0, 'è´Ÿå‘': 0, 'å·®': 0, 'ä¸å¥½': 0, 'å': 0,
        'æ­£é¢': 1, 'ç§¯æ': 1, 'æ­£å‘': 1, 'å¥½': 1, 'å¾ˆå¥½': 1, 'æ£’': 1,
        'ä¸­æ€§': 'neutral', 'ä¸­ç«‹': 'neutral', 'ä¸€èˆ¬': 'neutral',
        
        # è‹±æ–‡æ ‡ç­¾
        'negative': 0, 'bad': 0, 'poor': 0,
        'positive': 1, 'good': 1, 'excellent': 1,
        'neutral': 'neutral',
        
        # æ•°å­—æ ‡ç­¾
        '0': 0, '1': 1, '2': 'neutral',
        0: 0, 1: 1, 2: 'neutral',
    }
    
    all_texts, all_labels = [], []
    neutral_texts = []
    total_rows = 0
    
    for path in tqdm(filepaths, desc="å¤„ç†æ–‡ä»¶", unit="æ–‡ä»¶"):
        if not os.path.exists(path):
            continue
            
        # å°è¯•ä¸åŒç¼–ç 
        df = None
        for encoding in ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']:
            try:
                df = pd.read_csv(path, encoding=encoding)
                print(f"  âœ… {os.path.basename(path)}: ä½¿ç”¨ç¼–ç  {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if df is None:
            print(f"  âŒ æ— æ³•è¯»å–: {path}")
            continue
        
        if 'content' not in df.columns or 'sentiment' not in df.columns:
            print(f"  âŒ ç¼ºå°‘å¿…è¦åˆ—: {path}")
            continue
        
        total_rows += len(df)
        texts = df['content'].astype(str).tolist()
        
        # å¤„ç†æ ‡ç­¾
        valid_indices = []
        neutral_indices = []
        
        for i, label in enumerate(tqdm(df['sentiment'], desc=f"å¤„ç†æ ‡ç­¾", leave=False)):
            if pd.isna(label):
                continue
            
            label_key = int(label) if isinstance(label, (int, float)) else str(label).strip().lower()
            mapped = label_mapping.get(label_key, None)
            
            if mapped == 'neutral':
                neutral_indices.append(i)
            elif mapped is not None:
                all_labels.append(mapped)
                valid_indices.append(i)
        
        all_texts.extend([texts[i] for i in valid_indices])
        neutral_texts.extend([texts[i] for i in neutral_indices])
    
    # å¤„ç†ä¸­æ€§æ•°æ®
    current_pos = sum(1 for label in all_labels if label == 1)
    current_neg = sum(1 for label in all_labels if label == 0)
    
    print(f"\nğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"  æ­£é¢æ ·æœ¬: {current_pos:,}")
    print(f"  è´Ÿé¢æ ·æœ¬: {current_neg:,}")  
    print(f"  ä¸­æ€§æ ·æœ¬: {len(neutral_texts):,}")
    print(f"  æ€»è¡Œæ•°: {total_rows:,}")
    
    if neutral_texts and neutral_strategy != 'exclude':
        print(f"\nğŸ”„ å¤„ç†ä¸­æ€§æ ·æœ¬...")
        
        if neutral_strategy == 'balance':
            # å¹³è¡¡ç­–ç•¥ï¼šåˆ†é…ç»™è¾ƒå°‘çš„ç±»åˆ«
            if current_pos < current_neg:
                all_texts.extend(neutral_texts)
                all_labels.extend([1] * len(neutral_texts))
                print(f"  âœ… {len(neutral_texts):,}ä¸ªä¸­æ€§æ ·æœ¬åˆ†é…ç»™æ­£é¢ç±»åˆ«")
            else:
                all_texts.extend(neutral_texts)
                all_labels.extend([0] * len(neutral_texts))
                print(f"  âœ… {len(neutral_texts):,}ä¸ªä¸­æ€§æ ·æœ¬åˆ†é…ç»™è´Ÿé¢ç±»åˆ«")
        elif neutral_strategy == 'split':
            # æŒ‰æ¯”ä¾‹åˆ†é…
            random.shuffle(neutral_texts)
            split_point = int(len(neutral_texts) * 0.7)
            pos_neutrals = neutral_texts[:split_point]
            neg_neutrals = neutral_texts[split_point:]
            
            all_texts.extend(pos_neutrals + neg_neutrals)
            all_labels.extend([1] * len(pos_neutrals) + [0] * len(neg_neutrals))
            print(f"  âœ… ä¸­æ€§æ ·æœ¬åˆ†é…: {len(pos_neutrals):,}ä¸ªç»™æ­£é¢, {len(neg_neutrals):,}ä¸ªç»™è´Ÿé¢")
    
    final_pos = sum(1 for label in all_labels if label == 1)
    final_neg = sum(1 for label in all_labels if label == 0)
    utilization = (len(all_texts) / total_rows * 100) if total_rows > 0 else 0
    
    print(f"\nğŸ“ˆ æœ€ç»ˆæ•°æ®ç»Ÿè®¡:")
    print(f"  æ­£é¢æ ·æœ¬: {final_pos:,}")
    print(f"  è´Ÿé¢æ ·æœ¬: {final_neg:,}")
    print(f"  æ€»æ ·æœ¬æ•°: {len(all_texts):,}")
    print(f"  æ•°æ®åˆ©ç”¨ç‡: {utilization:.1f}%")
    
    return all_texts, all_labels

def create_sentiment_files(texts, labels, pos_path, neg_path):
    """åˆ›å»ºè¯­æ–™æ–‡ä»¶"""
    from tqdm import tqdm
    
    print(f"\nğŸ“ åˆ›å»ºè¯­æ–™æ–‡ä»¶...")
    os.makedirs(os.path.dirname(pos_path), exist_ok=True)
    os.makedirs(os.path.dirname(neg_path), exist_ok=True)
    
    with open(pos_path, 'w', encoding='utf-8') as f_pos, \
         open(neg_path, 'w', encoding='utf-8') as f_neg:
        
        pos_count, neg_count = 0, 0
        
        for text, label in tqdm(zip(texts, labels), total=len(texts), desc="åˆ›å»ºè¯­æ–™"):
            clean_text = text.replace('\n', '').replace('\r', '').strip()
            if len(clean_text) > 0:
                if label == 1:
                    f_pos.write(clean_text + '\n')
                    pos_count += 1
                elif label == 0:
                    f_neg.write(clean_text + '\n')
                    neg_count += 1
    
    print(f"  âœ… æ­£é¢è¯­æ–™: {pos_count:,} ä¸ªæ ·æœ¬")
    print(f"  âœ… è´Ÿé¢è¯­æ–™: {neg_count:,} ä¸ªæ ·æœ¬")
    
    return pos_count, neg_count

def train_model(neg_path, pos_path):
    """è®­ç»ƒæ¨¡å‹"""
    from snownlp import sentiment
    from tqdm import tqdm
    
    print(f"\nğŸ§  å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    print("âš ï¸  è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    start_time = time.time()
    
    try:
        # æ˜¾ç¤ºè¿›åº¦æç¤º
        print("ğŸ”„ SnowNLPæ ¸å¿ƒç®—æ³•è®­ç»ƒä¸­...")
        sentiment.train(neg_path, pos_path)
        
        elapsed = time.time() - start_time
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ! è€—æ—¶: {elapsed:.1f}ç§’")
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶
        model_files = []
        for pattern in ['*.marshal*', 'custom_sentiment.*']:
            model_files.extend(glob(pattern))
        
        if model_files:
            largest_file = max(model_files, key=os.path.getsize)
            size = os.path.getsize(largest_file)
            print(f"ğŸ“¦ æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {largest_file} ({size:,} å­—èŠ‚)")
            return largest_file
        else:
            print("âŒ æœªæ‰¾åˆ°ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶")
            return None
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        return None

def replace_model(model_file):
    """æ›¿æ¢ç³»ç»Ÿæ¨¡å‹"""
    print(f"\nğŸ”„ éƒ¨ç½²æ–°æ¨¡å‹...")
    
    try:
        import snownlp
        snownlp_dir = os.path.dirname(snownlp.__file__)
        sentiment_dir = os.path.join(snownlp_dir, 'sentiment')
        
        print(f"ğŸ“ SnowNLPç›®å½•: {sentiment_dir}")
        
        # æŸ¥æ‰¾ç›®æ ‡æ–‡ä»¶
        target_files = []
        for fname in ['sentiment.marshal', 'sentiment.marshal.3']:
            fpath = os.path.join(sentiment_dir, fname)
            if os.path.exists(fpath):
                target_files.append(fpath)
        
        if not target_files:
            print("âŒ æœªæ‰¾åˆ°ç›®æ ‡æ¨¡å‹æ–‡ä»¶")
            return False
        
        # å¤‡ä»½åŸæ–‡ä»¶
        backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        for target_file in target_files:
            backup_file = f"{target_file}.backup_{backup_time}"
            shutil.copy2(target_file, backup_file)
            print(f"ğŸ“‹ å¤‡ä»½: {os.path.basename(backup_file)}")
        
        # æ›¿æ¢æ¨¡å‹
        success_count = 0
        for target_file in target_files:
            try:
                shutil.copy2(model_file, target_file)
                size = os.path.getsize(target_file)
                print(f"âœ… æ›¿æ¢: {os.path.basename(target_file)} ({size:,} å­—èŠ‚)")
                success_count += 1
            except Exception as e:
                print(f"âŒ æ›¿æ¢å¤±è´¥ {os.path.basename(target_file)}: {e}")
        
        return success_count > 0
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹éƒ¨ç½²å¤±è´¥: {e}")
        return False

def evaluate_model(texts, labels, sample_size=1000):
    """è¯„ä¼°æ¨¡å‹"""
    from snownlp import SnowNLP
    from tqdm import tqdm
    
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
    
    # å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œéšæœºé‡‡æ ·
    if len(texts) > sample_size:
        print(f"ğŸ“ æ•°æ®é‡è¾ƒå¤§ï¼Œéšæœºé‡‡æ · {sample_size:,} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°")
        indices = random.sample(range(len(texts)), sample_size)
        eval_texts = [texts[i] for i in indices]
        eval_labels = [labels[i] for i in indices]
    else:
        eval_texts = texts
        eval_labels = labels
    
    correct = 0
    total = len(eval_texts)
    
    for text, true_label in tqdm(zip(eval_texts, eval_labels), 
                                total=total, desc="è¯„ä¼°æ¨¡å‹"):
        try:
            s = SnowNLP(text)
            score = s.sentiments
            pred_label = 1 if score > 0.5 else 0
            
            if pred_label == true_label:
                correct += 1
        except:
            continue
    
    accuracy = correct / total if total > 0 else 0
    
    print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
    print(f"  æµ‹è¯•æ ·æœ¬: {total:,}")
    print(f"  æ­£ç¡®é¢„æµ‹: {correct:,}")
    print(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
    
    if accuracy >= 0.8:
        print("ğŸ‰ æ¨¡å‹è¡¨ç°ä¼˜ç§€!")
    elif accuracy >= 0.6:
        print("ğŸ‘ æ¨¡å‹è¡¨ç°è‰¯å¥½!")
    else:
        print("ğŸ˜ æ¨¡å‹éœ€è¦æ”¹è¿›")
    
    return accuracy

def interactive_test():
    """äº¤äº’å¼æµ‹è¯•"""
    from snownlp import SnowNLP
    
    print(f"\nğŸ® äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
    print("è¾“å…¥æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("-" * 50)
    
    while True:
        try:
            text = input("\nè¯·è¾“å…¥æµ‹è¯•æ–‡æœ¬: ").strip()
            
            if text.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ é€€å‡ºæµ‹è¯•æ¨¡å¼")
                break
            
            if not text:
                continue
            
            s = SnowNLP(text)
            score = s.sentiments
            
            if score > 0.6:
                sentiment = "æ­£é¢ ğŸ˜Š"
                color = "ç»¿è‰²"
            elif score < 0.4:
                sentiment = "è´Ÿé¢ ğŸ˜"
                color = "çº¢è‰²"
            else:
                sentiment = "ä¸­æ€§ ğŸ˜"
                color = "é»„è‰²"
            
            print(f"ğŸ“Š åˆ†æç»“æœ:")
            print(f"  å¾—åˆ†: {score:.4f}")
            print(f"  æƒ…æ„Ÿ: {sentiment}")
            
            if score > 0.8:
                print("  å¼ºåº¦: å¼ºçƒˆæ­£é¢")
            elif score < 0.2:
                print("  å¼ºåº¦: å¼ºçƒˆè´Ÿé¢")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ é€€å‡ºæµ‹è¯•æ¨¡å¼")
            break
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")

def quick_test():
    """å¿«é€ŸéªŒè¯æµ‹è¯•"""
    from snownlp import SnowNLP
    
    print(f"\nâš¡ å¿«é€ŸéªŒè¯æµ‹è¯•")
    
    test_cases = [
        ("è¿™ä¸ªäº§å“è´¨é‡éå¸¸å¥½ï¼Œå¼ºçƒˆæ¨èï¼", "æ­£é¢"),
        ("æœåŠ¡æ€åº¦å¤ªå·®äº†ï¼Œå¾ˆä¸æ»¡æ„", "è´Ÿé¢"),
        ("è¿˜å¯ä»¥å§ï¼Œä¸€èˆ¬èˆ¬", "ä¸­æ€§"),
        ("ç‰©æµé€Ÿåº¦å¾ˆå¿«ï¼ŒåŒ…è£…ä¹Ÿä¸é”™", "æ­£é¢"),
        ("ä»·æ ¼æœ‰ç‚¹è´µï¼Œä½†è´¨é‡ç¡®å®å¥½", "æ­£é¢"),
        ("ç”¨äº†å‡ å¤©å°±åäº†ï¼Œå¤ªå¤±æœ›", "è´Ÿé¢"),
        ("æ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—è´­ä¹°", "æ­£é¢"),
        ("å®¢æœæ€åº¦æ¶åŠ£ï¼Œå¾ˆç”Ÿæ°”", "è´Ÿé¢")
    ]
    
    correct = 0
    total = 0
    
    for i, (text, expected) in enumerate(test_cases, 1):
        try:
            s = SnowNLP(text)
            score = s.sentiments
            
            if score > 0.6:
                predicted = "æ­£é¢"
            elif score < 0.4:
                predicted = "è´Ÿé¢"
            else:
                predicted = "ä¸­æ€§"
            
            is_correct = predicted == expected or expected == "ä¸­æ€§"
            if expected != "ä¸­æ€§":
                total += 1
                if is_correct:
                    correct += 1
            
            status = "âœ…" if is_correct else "âŒ"
            print(f"{status} [{i}] {score:.4f} ({predicted}) | {text}")
            
        except Exception as e:
            print(f"âŒ [{i}] æµ‹è¯•å¤±è´¥: {e}")
    
    if total > 0:
        accuracy = correct / total
        print(f"\nğŸ“Š å¿«é€Ÿæµ‹è¯•ç»“æœ: {correct}/{total} æ­£ç¡®ï¼Œå‡†ç¡®ç‡: {accuracy:.2%}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="SnowNLPæƒ…æ„Ÿåˆ†æè®­ç»ƒå·¥å…· - å‘½ä»¤è¡Œç‰ˆæœ¬")
    parser.add_argument('--train', action='store_true', help='æ‰§è¡Œæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--test', action='store_true', help='å¿«é€ŸéªŒè¯æµ‹è¯•')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’å¼æµ‹è¯•')
    parser.add_argument('--eval', action='store_true', help='æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--neutral-strategy', choices=['balance', 'split', 'exclude'], 
                       default='balance', help='ä¸­æ€§æ•°æ®å¤„ç†ç­–ç•¥')
    
    args = parser.parse_args()
    
    print_banner()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return 1
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå‚æ•°ï¼Œæ˜¾ç¤ºèœå•
    if not any([args.train, args.test, args.interactive, args.eval]):
        while True:
            print(f"\nğŸ¯ è¯·é€‰æ‹©æ“ä½œ:")
            print("1. ğŸš€ è®­ç»ƒæ–°æ¨¡å‹")
            print("2. âš¡ å¿«é€ŸéªŒè¯æµ‹è¯•")
            print("3. ğŸ“Š æ¨¡å‹è¯„ä¼°") 
            print("4. ğŸ® äº¤äº’å¼æµ‹è¯•")
            print("5. ğŸ” æŸ¥çœ‹æ•°æ®æ–‡ä»¶ä¿¡æ¯")
            print("0. ğŸšª é€€å‡ºç¨‹åº")
            
            try:
                choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-5): ").strip()
                
                if choice == '0':
                    print("ğŸ‘‹ å†è§!")
                    break
                elif choice == '1':
                    args.train = True
                    break
                elif choice == '2':
                    args.test = True
                    break
                elif choice == '3':
                    args.eval = True
                    break
                elif choice == '4':
                    args.interactive = True
                    break
                elif choice == '5':
                    find_data_files()
                    continue
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§!")
                break
    
    # æ‰§è¡Œé€‰æ‹©çš„æ“ä½œ
    if args.train:
        # è®­ç»ƒæ¨¡å‹
        train_files, test_files = find_data_files()
        
        if not train_files:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
            return 1
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_texts, train_labels = load_data_with_progress(
            train_files, "è®­ç»ƒ", args.neutral_strategy)
        
        if not train_texts:
            print("âŒ è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥")
            return 1
        
        # åˆ›å»ºè¯­æ–™æ–‡ä»¶
        pos_path = 'temp_data/pos.txt'
        neg_path = 'temp_data/neg.txt'
        pos_count, neg_count = create_sentiment_files(train_texts, train_labels, pos_path, neg_path)
        
        if pos_count == 0 or neg_count == 0:
            print("âŒ æ­£é¢æˆ–è´Ÿé¢æ ·æœ¬æ•°é‡ä¸º0ï¼Œæ— æ³•è®­ç»ƒ")
            return 1
        
        # è®­ç»ƒæ¨¡å‹
        model_file = train_model(neg_path, pos_path)
        if not model_file:
            return 1
        
        # æ›¿æ¢æ¨¡å‹
        if replace_model(model_file):
            print(f"\nğŸ‰ æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²å®Œæˆ!")
            
            # å¦‚æœæœ‰æµ‹è¯•æ•°æ®ï¼Œè¿›è¡Œè¯„ä¼°
            if test_files:
                test_texts, test_labels = load_data_with_progress(test_files, "æµ‹è¯•")
                if test_texts:
                    evaluate_model(test_texts, test_labels)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists('temp_data'):
                shutil.rmtree('temp_data')
        except:
            pass
    
    elif args.test:
        quick_test()
    
    elif args.eval:
        train_files, test_files = find_data_files()
        if test_files:
            test_texts, test_labels = load_data_with_progress(test_files, "æµ‹è¯•")
            if test_texts:
                evaluate_model(test_texts, test_labels)
        else:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶")
    
    elif args.interactive:
        interactive_test()
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 