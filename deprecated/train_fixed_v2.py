# -*- coding: utf-8 -*-
"""
SnowNLPæƒ…æ„Ÿåˆ†æè®­ç»ƒè„šæœ¬ v2.0
å®Œå…¨ç»•è¿‡sentiment.save()é—®é¢˜çš„ç‰ˆæœ¬
ç›´æ¥æ“ä½œSnowNLPå†…éƒ¨æ¨¡å‹æ–‡ä»¶
"""

import pandas as pd
import os
import time
import sys
import shutil
from snownlp import sentiment
from snownlp.sentiment import Sentiment
from glob import glob
from tqdm import tqdm
import random
import marshal
import pickle

def load_multiple_csvs(filepaths, text_col='content', label_col='sentiment', neutral_strategy='balance'):
    """
    åŠ è½½å¤šä¸ªCSVæ–‡ä»¶å¹¶åˆå¹¶æ•°æ®ï¼Œæ”¯æŒå¤šç§ä¸­æ€§æ•°æ®å¤„ç†ç­–ç•¥
    """
    def detect_encoding(file_path):
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            import chardet
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)
                result = chardet.detect(raw_data)
                return result['encoding']
        except ImportError:
            encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin1']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        f.read(1000)
                    return encoding
                except UnicodeDecodeError:
                    continue
            return 'utf-8'
    
    def read_csv_with_encoding(file_path):
        """ä½¿ç”¨æ­£ç¡®ç¼–ç è¯»å–CSVæ–‡ä»¶"""
        try:
            return pd.read_csv(file_path, encoding='utf-8')
        except UnicodeDecodeError:
            print(f"UTF-8ç¼–ç å¤±è´¥ï¼Œæ­£åœ¨æ£€æµ‹æ–‡ä»¶ç¼–ç : {file_path}")
            
        detected_encoding = detect_encoding(file_path)
        print(f"æ£€æµ‹åˆ°ç¼–ç : {detected_encoding}")
        
        try:
            return pd.read_csv(file_path, encoding=detected_encoding)
        except UnicodeDecodeError:
            print(f"æ£€æµ‹ç¼–ç å¤±è´¥ï¼Œå°è¯•å¸¸è§ç¼–ç ...")
            
        encodings = ['gbk', 'gb2312', 'utf-8-sig', 'latin1', 'cp1252']
        for encoding in encodings:
            try:
                print(f"å°è¯•ç¼–ç : {encoding}")
                return pd.read_csv(file_path, encoding=encoding)
            except UnicodeDecodeError:
                continue
        
        print("æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶å¿½ç•¥é”™è¯¯")
        return pd.read_csv(file_path, encoding='utf-8', errors='ignore')
    
    # æ ‡ç­¾æ˜ å°„
    label_mapping = {
        'è´Ÿé¢': 0, 'æ¶ˆæ': 0, 'è´Ÿå‘': 0, 'negative': 0,
        'æ­£é¢': 1, 'ç§¯æ': 1, 'æ­£å‘': 1, 'positive': 1,
        'ä¸­æ€§': 'neutral', 'ä¸­ç«‹': 'neutral', 'neutral': 'neutral'
    }

    all_texts, all_labels = [], []
    neutral_texts = []
    print(f"å¼€å§‹åŠ è½½ {len(filepaths)} ä¸ªæ•°æ®æ–‡ä»¶...")
    print(f"ä¸­æ€§æ•°æ®å¤„ç†ç­–ç•¥: {neutral_strategy}")

    for path in tqdm(filepaths, desc="åŠ è½½æ–‡ä»¶"):
        if not os.path.exists(path):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            continue
        
        try:
            print(f"\næ­£åœ¨åŠ è½½æ–‡ä»¶: {path}")
            df = read_csv_with_encoding(path)
            print(f"æˆåŠŸåŠ è½½ï¼Œå…± {len(df)} è¡Œæ•°æ®")
            
            if text_col not in df.columns:
                print(f"è­¦å‘Š: åˆ— '{text_col}' ä¸å­˜åœ¨äºæ–‡ä»¶ {path}")
                print(f"å¯ç”¨åˆ—: {list(df.columns)}")
                continue
                
            if label_col not in df.columns:
                print(f"è­¦å‘Š: åˆ— '{label_col}' ä¸å­˜åœ¨äºæ–‡ä»¶ {path}")
                print(f"å¯ç”¨åˆ—: {list(df.columns)}")
                continue
                
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {path}: {e}")
            continue
            
        texts = df[text_col].astype(str).tolist()
        labels = []
        valid_indices = []
        neutral_indices = []

        for i, label in enumerate(df[label_col]):
            label_str = str(label).strip().lower()
            mapped = label_mapping.get(label_str, None)

            if mapped == 'neutral':
                neutral_indices.append(i)
            elif mapped is not None:
                labels.append(mapped)
                valid_indices.append(i)

        all_texts.extend([texts[i] for i in valid_indices])
        all_labels.extend(labels)
        neutral_texts.extend([texts[i] for i in neutral_indices])

    # å¤„ç†ä¸­æ€§æ•°æ®
    current_pos = sum(1 for label in all_labels if label == 1)
    current_neg = sum(1 for label in all_labels if label == 0)
    neutral_count = len(neutral_texts)
    
    print(f"åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"  æ­£é¢æ ·æœ¬: {current_pos}")
    print(f"  è´Ÿé¢æ ·æœ¬: {current_neg}")  
    print(f"  ä¸­æ€§æ ·æœ¬: {neutral_count}")

    if neutral_count > 0 and neutral_strategy != 'exclude':
        print(f"æ­£åœ¨å¤„ç† {neutral_count} ä¸ªä¸­æ€§æ ·æœ¬...")
        
        if neutral_strategy == 'random':
            for text in neutral_texts:
                label = random.choice([0, 1])
                all_texts.append(text)
                all_labels.append(label)
        elif neutral_strategy == 'balance':
            if current_pos < current_neg:
                for text in neutral_texts:
                    all_texts.append(text)
                    all_labels.append(1)
                print(f"  ä¸­æ€§æ ·æœ¬å…¨éƒ¨åˆ†é…ç»™æ­£é¢ç±»åˆ«(ç”¨äºå¹³è¡¡)")
            else:
                for text in neutral_texts:
                    all_texts.append(text)
                    all_labels.append(0)
                print(f"  ä¸­æ€§æ ·æœ¬å…¨éƒ¨åˆ†é…ç»™è´Ÿé¢ç±»åˆ«(ç”¨äºå¹³è¡¡)")
        elif neutral_strategy == 'positive':
            for text in neutral_texts:
                all_texts.append(text)
                all_labels.append(1)
            print(f"  ä¸­æ€§æ ·æœ¬å…¨éƒ¨åˆ†é…ç»™æ­£é¢ç±»åˆ«")
        elif neutral_strategy == 'negative':
            for text in neutral_texts:
                all_texts.append(text)
                all_labels.append(0)
            print(f"  ä¸­æ€§æ ·æœ¬å…¨éƒ¨åˆ†é…ç»™è´Ÿé¢ç±»åˆ«")
        elif neutral_strategy == 'split':
            random.shuffle(neutral_texts)
            split_point = int(len(neutral_texts) * 0.7)
            pos_neutrals = neutral_texts[:split_point]
            neg_neutrals = neutral_texts[split_point:]
            
            for text in pos_neutrals:
                all_texts.append(text)
                all_labels.append(1)
            for text in neg_neutrals:
                all_texts.append(text)
                all_labels.append(0)
                
            print(f"  ä¸­æ€§æ ·æœ¬æŒ‰æ¯”ä¾‹åˆ†é…: {len(pos_neutrals)}ä¸ªç»™æ­£é¢, {len(neg_neutrals)}ä¸ªç»™è´Ÿé¢")

    final_pos = sum(1 for label in all_labels if label == 1)
    final_neg = sum(1 for label in all_labels if label == 0)
    
    print(f"æœ€ç»ˆæ•°æ®ç»Ÿè®¡:")
    print(f"  æ­£é¢æ ·æœ¬: {final_pos}")
    print(f"  è´Ÿé¢æ ·æœ¬: {final_neg}")
    print(f"  æ€»æ ·æœ¬æ•°: {len(all_texts)}")

    return all_texts, all_labels

def create_sentiment_files(texts, labels, pos_path, neg_path):
    """åˆ›å»ºæƒ…æ„Ÿåˆ†æè¯­æ–™æ–‡ä»¶"""
    os.makedirs(os.path.dirname(pos_path), exist_ok=True)
    os.makedirs(os.path.dirname(neg_path), exist_ok=True)

    with open(pos_path, 'w', encoding='utf-8') as f_pos, \
         open(neg_path, 'w', encoding='utf-8') as f_neg:

        print("åˆ›å»ºæƒ…æ„Ÿè¯­æ–™æ–‡ä»¶...")
        pos_count, neg_count = 0, 0

        for text, label in tqdm(zip(texts, labels), total=len(texts), desc="å¤„ç†æ ·æœ¬"):
            clean_text = text.replace('\n', '').replace('\r', '').strip()
            if len(clean_text) > 0:
                if label == 1:
                    f_pos.write(clean_text + '\n')
                    pos_count += 1
                elif label == 0:
                    f_neg.write(clean_text + '\n')
                    neg_count += 1

        print(f"åˆ›å»ºå®Œæˆ: {pos_count} ä¸ªç§¯ææ ·æœ¬, {neg_count} ä¸ªæ¶ˆææ ·æœ¬")
        return pos_count, neg_count

def evaluate_model_with_snownlp(test_texts, test_labels):
    """ä½¿ç”¨SnowNLPè¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡"""
    from snownlp import SnowNLP
    
    print("ä½¿ç”¨ä¼ ç»ŸäºŒåˆ†ç±»è¯„ä¼° (è´Ÿé¢ < 0.5, æ­£é¢ >= 0.5)")
    
    correct = 0
    total = len(test_texts)
    
    for text, label in tqdm(zip(test_texts, test_labels), total=total, desc="äºŒåˆ†ç±»è¯„ä¼°"):
        try:
            s = SnowNLP(text)
            score = s.sentiments
            pred_label = 1 if score > 0.5 else 0
            if pred_label == label:
                correct += 1
        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥: {e}")
            continue

    return correct / total if total > 0 else 0

def direct_replace_snownlp_model(neg_path, pos_path):
    """
    ç›´æ¥æ›¿æ¢SnowNLPç³»ç»Ÿæ¨¡å‹ï¼Œç»•è¿‡save()æ–¹æ³•
    è¿™æ˜¯ä¸€ä¸ªhackæ–¹æ³•ï¼Œç›´æ¥æ“ä½œSnowNLPçš„å†…éƒ¨æ–‡ä»¶
    """
    print("\n" + "="*50)
    print("ğŸ”§ å¼€å§‹ç›´æ¥æ¨¡å‹æ›¿æ¢ï¼ˆç»•è¿‡saveæ–¹æ³•ï¼‰...")
    
    try:
        # è·å–SnowNLPå®‰è£…è·¯å¾„
        import snownlp
        snownlp_path = os.path.dirname(snownlp.__file__)
        sentiment_path = os.path.join(snownlp_path, 'sentiment')
        
        print(f"SnowNLPè·¯å¾„: {snownlp_path}")
        print(f"Sentimentæ¨¡å—è·¯å¾„: {sentiment_path}")
        
        # å¤‡ä»½åŸå§‹æ¨¡å‹
        model_files = []
        for ext in ['', '.3', '.2']:
            model_file = os.path.join(sentiment_path, f'sentiment.marshal{ext}')
            if os.path.exists(model_file):
                model_files.append(model_file)
                backup_file = model_file + '.backup_v2'
                if not os.path.exists(backup_file):
                    shutil.copy2(model_file, backup_file)
                    print(f"âœ… å·²å¤‡ä»½: {backup_file}")
        
        if not model_files:
            print("âŒ æœªæ‰¾åˆ°åŸå§‹æ¨¡å‹æ–‡ä»¶")
            return False
        
        # ä¸´æ—¶è®­ç»ƒæ–°æ¨¡å‹
        print("å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹...")
        sentiment.train(neg_path, pos_path)
        print("âœ… è®­ç»ƒå®Œæˆ")
        
        # å°è¯•æ–¹æ³•1ï¼šç›´æ¥è·å–è®­ç»ƒåçš„æ¨¡å‹æ•°æ®
        try:
            print("å°è¯•æ–¹æ³•1ï¼šç›´æ¥æ¨¡å‹æ•°æ®æå–...")
            
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è§¦å‘æ¨¡å‹è®­ç»ƒå¹¶è·å–ç»“æœ
            # ç”±äºSnowNLPçš„å†…éƒ¨å®ç°ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°å®ç°è®­ç»ƒé€»è¾‘
            
            from snownlp.sentiment.sentiment import train as train_func
            from snownlp.sentiment import data_path as sentiment_data_path
            
            # é‡æ–°è®­ç»ƒå¹¶è·å–æ¨¡å‹
            model_data = train_func(neg_path, pos_path)
            
            if model_data:
                # ä¿å­˜åˆ°æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
                for model_file in model_files:
                    try:
                        print(f"ä¿å­˜æ¨¡å‹åˆ°: {model_file}")
                        with open(model_file, 'wb') as f:
                            marshal.dump(model_data, f)
                        print(f"âœ… æˆåŠŸä¿å­˜: {model_file}")
                    except Exception as e:
                        print(f"âŒ ä¿å­˜å¤±è´¥ {model_file}: {e}")
                
                return True
            else:
                print("âŒ æœªèƒ½è·å–æ¨¡å‹æ•°æ®")
                
        except Exception as e:
            print(f"æ–¹æ³•1å¤±è´¥: {e}")
        
        # å°è¯•æ–¹æ³•2ï¼šå¤åˆ¶ä¸´æ—¶ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶
        try:
            print("å°è¯•æ–¹æ³•2ï¼šæŸ¥æ‰¾ä¸´æ—¶æ¨¡å‹æ–‡ä»¶...")
            
            # æŸ¥æ‰¾å¯èƒ½çš„ä¸´æ—¶æ¨¡å‹æ–‡ä»¶ä½ç½®
            from snownlp.sentiment import data_path as sentiment_data_path
            
            possible_temp_files = [
                os.path.join(sentiment_data_path, 'sentiment.marshal'),
                os.path.join(sentiment_data_path, 'sentiment.marshal.3'),
                'sentiment.marshal',
                'sentiment.marshal.3',
                os.path.join(os.getcwd(), 'sentiment.marshal'),
                os.path.join(os.getcwd(), 'sentiment.marshal.3')
            ]
            
            for temp_file in possible_temp_files:
                if os.path.exists(temp_file):
                    print(f"æ‰¾åˆ°ä¸´æ—¶æ¨¡å‹: {temp_file}")
                    file_size = os.path.getsize(temp_file)
                    if file_size > 0:
                        # å¤åˆ¶åˆ°ç³»ç»Ÿä½ç½®
                        for model_file in model_files:
                            try:
                                shutil.copy2(temp_file, model_file)
                                print(f"âœ… å¤åˆ¶æˆåŠŸ: {temp_file} â†’ {model_file}")
                            except Exception as e:
                                print(f"âŒ å¤åˆ¶å¤±è´¥: {e}")
                        return True
                    
        except Exception as e:
            print(f"æ–¹æ³•2å¤±è´¥: {e}")
        
        # å°è¯•æ–¹æ³•3ï¼šæ‰‹åŠ¨é‡å»ºè®­ç»ƒè¿‡ç¨‹
        try:
            print("å°è¯•æ–¹æ³•3ï¼šæ‰‹åŠ¨é‡å»ºè®­ç»ƒ...")
            
            # è¯»å–è®­ç»ƒæ•°æ®
            pos_data = []
            neg_data = []
            
            with open(pos_path, 'r', encoding='utf-8') as f:
                pos_data = [line.strip() for line in f if line.strip()]
            
            with open(neg_path, 'r', encoding='utf-8') as f:
                neg_data = [line.strip() for line in f if line.strip()]
            
            print(f"è¯»å–è®­ç»ƒæ•°æ®: {len(pos_data)} æ­£é¢, {len(neg_data)} è´Ÿé¢")
            
            # ä½¿ç”¨sklearné‡æ–°è®­ç»ƒ
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                from sklearn.naive_bayes import MultinomialNB
                import jieba
                
                # å‡†å¤‡æ•°æ®
                all_texts = pos_data + neg_data
                all_labels = [1] * len(pos_data) + [0] * len(neg_data)
                
                # åˆ†è¯
                print("æ­£åœ¨åˆ†è¯...")
                segmented_texts = []
                for text in all_texts[:5000]:  # é™åˆ¶æ ·æœ¬æ•°é‡åŠ å¿«é€Ÿåº¦
                    words = list(jieba.cut(text))
                    segmented_texts.append(' '.join(words))
                
                # å¯¹åº”çš„æ ‡ç­¾ä¹Ÿè¦æˆªå–
                limited_labels = all_labels[:5000]
                
                # è®­ç»ƒæ¨¡å‹
                print("è®­ç»ƒsklearnæ¨¡å‹...")
                vectorizer = TfidfVectorizer(max_features=3000)
                X = vectorizer.fit_transform(segmented_texts)
                
                classifier = MultinomialNB()
                classifier.fit(X, limited_labels)
                
                # ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹
                model_package = {
                    'vectorizer': vectorizer,
                    'classifier': classifier,
                    'version': 'custom_v2'
                }
                
                # å…ˆå°è¯•ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_model_path = 'custom_model_temp.pkl'
                with open(temp_model_path, 'wb') as f:
                    pickle.dump(model_package, f)
                
                # éªŒè¯ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_model_path) and os.path.getsize(temp_model_path) > 0:
                    print(f"âœ… ä¸´æ—¶æ¨¡å‹åˆ›å»ºæˆåŠŸ: {temp_model_path}")
                    
                    # å¤åˆ¶åˆ°ç³»ç»Ÿä½ç½®
                    for model_file in model_files:
                        try:
                            shutil.copy2(temp_model_path, model_file)
                            print(f"âœ… å¤åˆ¶æˆåŠŸ: {temp_model_path} â†’ {model_file}")
                        except Exception as e:
                            print(f"âŒ å¤åˆ¶å¤±è´¥: {e}")
                    
                    return True
                else:
                    print("âŒ ä¸´æ—¶æ¨¡å‹æ–‡ä»¶åˆ›å»ºå¤±è´¥")
                    
            except ImportError as e:
                print(f"ç¼ºå°‘ä¾èµ–åº“: {e}")
                print("è¯·è¿è¡Œ: pip install scikit-learn jieba")
                
        except Exception as e:
            print(f"æ–¹æ³•3å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print("âŒ æ‰€æœ‰æ¨¡å‹æ›¿æ¢æ–¹æ³•éƒ½å¤±è´¥äº†")
        return False
        
    except Exception as e:
        print(f"âŒ ç›´æ¥æ¨¡å‹æ›¿æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    print("SnowNLPæƒ…æ„Ÿåˆ†æè®­ç»ƒè„šæœ¬ v2.0")
    print("ç»•è¿‡sentiment.save()é—®é¢˜çš„ç‰ˆæœ¬")
    print("="*50)

    # ä¸­æ€§æ•°æ®å¤„ç†ç­–ç•¥é€‰æ‹©
    print("ä¸­æ€§æ•°æ®å¤„ç†ç­–ç•¥é€‰é¡¹:")
    print("1. balance  - è‡ªåŠ¨å¹³è¡¡(æ¨è)")
    print("2. random   - éšæœºåˆ†é…")
    print("3. positive - å…¨éƒ¨æ­£é¢")
    print("4. negative - å…¨éƒ¨è´Ÿé¢") 
    print("5. split    - æ¯”ä¾‹åˆ†é…")
    print("6. exclude  - æ’é™¤ä¸­æ€§")
    
    strategy_map = {
        '1': 'balance', '2': 'random', '3': 'positive', 
        '4': 'negative', '5': 'split', '6': 'exclude'
    }
    
    while True:
        choice = input("\nè¯·é€‰æ‹©ä¸­æ€§æ•°æ®å¤„ç†ç­–ç•¥ (1-6ï¼Œé»˜è®¤ä¸º1): ").strip()
        if choice == "":
            choice = "1"
        if choice in strategy_map:
            neutral_strategy = strategy_map[choice]
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-6")

    print(f"å·²é€‰æ‹©ç­–ç•¥: {neutral_strategy}")

    # æ•°æ®æ–‡ä»¶æ£€æŸ¥
    train_files = ['train.csv', 'è®­ç»ƒé›†.csv']
    existing_files = [f for f in train_files if os.path.exists(f)]
    
    if not existing_files:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
        print("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š", train_files)
        return

    print(f"æ‰¾åˆ°è®­ç»ƒæ–‡ä»¶: {existing_files}")

    # åŠ è½½è®­ç»ƒæ•°æ®
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_texts, train_labels = load_multiple_csvs(existing_files, neutral_strategy=neutral_strategy)
    
    if len(train_texts) == 0:
        print("é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        return

    # åˆ›å»ºè¯­æ–™æ–‡ä»¶
    pos_path = 'temp_data/pos.txt'
    neg_path = 'temp_data/neg.txt'
    pos_count, neg_count = create_sentiment_files(train_texts, train_labels, pos_path, neg_path)
    
    if pos_count == 0 or neg_count == 0:
        print("é”™è¯¯ï¼šæ­£é¢æˆ–è´Ÿé¢æ ·æœ¬æ•°é‡ä¸º0ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
        return

    # è®­ç»ƒå‰æµ‹è¯•
    test_files = ['test.csv']
    existing_test_files = [f for f in test_files if os.path.exists(f)]
    
    if existing_test_files:
        print("åŠ è½½æµ‹è¯•æ•°æ®...")
        test_texts, test_labels = load_multiple_csvs(existing_test_files, neutral_strategy=neutral_strategy)
        
        if test_texts:
            print("\nå¼€å§‹è®­ç»ƒå‰æµ‹è¯•...")
            base_acc = evaluate_model_with_snownlp(test_texts, test_labels)
            print(f"ã€è®­ç»ƒå‰ã€‘æ¨¡å‹å‡†ç¡®ç‡ï¼š{base_acc:.2%}")

    # ç›´æ¥æ¨¡å‹æ›¿æ¢
    success = direct_replace_snownlp_model(neg_path, pos_path)
    
    if success:
        print("\nâœ… æ¨¡å‹è®­ç»ƒå’Œæ›¿æ¢æˆåŠŸï¼")
        print("é‡è¦æç¤ºï¼š")
        print("1. è¯·é‡å¯Pythonè§£é‡Šå™¨")
        print("2. é‡æ–°å¯¼å…¥snownlpåº“")
        print("3. æµ‹è¯•æ–°æ¨¡å‹æ•ˆæœ")
        
        if existing_test_files and test_texts:
            print("\nå»ºè®®æµ‹è¯•ä»£ç ï¼š")
            print("""
from snownlp import SnowNLP
test_text = "è¿™ä¸ªäº§å“å¾ˆå¥½ç”¨"
s = SnowNLP(test_text)
print(f"æƒ…æ„Ÿå¾—åˆ†: {s.sentiments:.4f}")
            """)
    else:
        print("\nâŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
        print("å»ºè®®ï¼š")
        print("1. æ£€æŸ¥æ•°æ®è´¨é‡")
        print("2. å°è¯•å‡å°‘è®­ç»ƒæ ·æœ¬æ•°é‡")
        print("3. ä½¿ç”¨å…¶ä»–æƒ…æ„Ÿåˆ†æåº“")

    # æ€»è€—æ—¶
    total_time = time.time() - start_time
    print(f"\næ€»è€—æ—¶: {total_time:.2f} ç§’")

if __name__ == "__main__":
    main() 